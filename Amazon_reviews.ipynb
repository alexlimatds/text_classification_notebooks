{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text classification example from: https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Stuning even for the non-gamer: This sound tra...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>The best soundtrack ever to anything.: I'm rea...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Amazing!: This soundtrack is my favorite music...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Excellent Soundtrack: I truly like this soundt...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       label\n",
       "0  Stuning even for the non-gamer: This sound tra...  __label__2\n",
       "1  The best soundtrack ever to anything.: I'm rea...  __label__2\n",
       "2  Amazing!: This soundtrack is my favorite music...  __label__2\n",
       "3  Excellent Soundtrack: I truly like this soundt...  __label__2\n",
       "4  Remember, Pull Your Jaw Off The Floor After He...  __label__2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('amazon_reviews.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df['text'], df['label'])\n",
    "\n",
    "# encoding the target variable\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Count vector\n",
    "\n",
    "- It doesn't remove stop words.\n",
    "- `analyzer='word'` word n-grams.\n",
    "- `token_pattern=r'\\w{1,}'` regular expression setting the token's separator (Any alpha, numeric, underline and with at least on character)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  31666\n",
      "CPU times: user 1.75 s, sys: 22.5 ms, total: 1.77 s\n",
      "Wall time: 1.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(df['text'])\n",
    "\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)\n",
    "\n",
    "print('Vocabulary size: ', len(count_vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 30916\n",
      "i 17124\n",
      "and 16162\n",
      "a 15462\n",
      "to 14451\n",
      "it 13682\n",
      "of 12726\n",
      "this 11686\n",
      "is 10956\n",
      "in 7525\n"
     ]
    }
   ],
   "source": [
    "# showing the 10 most frequent terms in training corpus\n",
    "sum_words = xtrain_count.sum(axis=0) \n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in count_vect.vocabulary_.items()]\n",
    "words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "for word, freq in words_freq[:10]:\n",
    "    print(word, freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF-IDF vector\n",
    "\n",
    "- It doesn't remove stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.9 s, sys: 211 ms, total: 22.1 s\n",
      "Wall time: 21.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(df['text'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(df['text'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(df['text'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word Embeddings\n",
    "\n",
    "Using pre-trained word embeddings from FastText: https://fasttext.cc/docs/en/english-vectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 55s, sys: 2 s, total: 1min 57s\n",
      "Wall time: 1min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "# load the pre-trained word-embedding vectors \n",
    "embeddings_index = {}\n",
    "for i, line in enumerate(open('wiki-news-300d-1M.vec')):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n",
    "\n",
    "# create a tokenizer \n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(df['text'])\n",
    "word_index = token.word_index\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "\n",
    "Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors:  0.826\n",
      "NB, WordLevel TF-IDF:  0.8352\n",
      "NB, N-Gram Vectors:  0.8352\n",
      "NB, CharLevel Vectors:  0.8104\n",
      "CPU times: user 149 ms, sys: 3.82 ms, total: 153 ms\n",
      "Wall time: 82.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn import naive_bayes\n",
    "\n",
    "# Naive Bayes on Count Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "print(\"NB, Count Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"NB, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print(\"NB, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print(\"NB, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, N-Gram Vectors:  0.4964\n",
      "CPU times: user 43.8 s, sys: 141 ms, total: 43.9 s\n",
      "Wall time: 44.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn import svm\n",
    "\n",
    "# SVM on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(gamma='auto'), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print(\"SVM, N-Gram Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, Count Vectors:  0.7972\n",
      "Xgb, WordLevel TF-IDF:  0.8\n",
      "Xgb, CharLevel Vectors:  0.8024\n",
      "CPU times: user 1min 7s, sys: 176 ms, total: 1min 7s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas, xgboost\n",
    "\n",
    "# Extereme Gradient Boosting on Count Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
    "print(\"Xgb, Count Vectors: \", accuracy)\n",
    "\n",
    "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
    "print(\"Xgb, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Extereme Gradient Boosting on Character Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), train_y, xvalid_tfidf_ngram_chars.tocsc())\n",
    "print(\"Xgb, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 15s 2ms/step - loss: 0.5864\n",
      "CNN, Word Embeddings 0.4964\n",
      "CPU times: user 35.1 s, sys: 19.3 s, total: 54.4 s\n",
      "Wall time: 17.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from keras import layers, models, optimizers\n",
    "\n",
    "def create_cnn():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_cnn()\n",
    "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print(\"CNN, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
